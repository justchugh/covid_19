{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3289716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2001a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0457eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffd723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef96f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13229e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06bd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7443883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd387b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('final.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfecfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.iloc[4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets['Tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75aebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_dict = {   \n",
    "    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "    \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I_would_have\", \"I'll\": \"I_will\", \"I'll've\": \"I_will_have\",\"I'm\": \"I am\", \n",
    "    \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "    \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "    \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n",
    "    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall_not\", \"shan't've\": \"shall_not_have\",\n",
    "    \"she'd\": \"she_would\", \"she'd've\": \"she_would_have\", \"she'll\": \"she_will\", \"she'll've\": \"she_will_have\", \n",
    "    \"she's\": \"she_is\", \"should've\": \"should_have\", \"shouldn't\": \"should_not\", \"shouldn't've\": \"should_not_have\", \n",
    "    \"so've\": \"so_have\",\"so's\": \"so_as\", \"this's\": \"this_is\",\"that'd\": \"that_would\", \"that'd've\": \"that_would_have\", \n",
    "    \"that's\": \"that is\", \"there'd\": \"there_would\", \"there'd've\": \"there_would_have\", \"there's\": \"there is\", \n",
    "    \"here's\": \"here is\",\"they'd\": \"they_would\", \"they'd've\": \"they_would_have\", \"they'll\": \"they_will\", \n",
    "    \"they'll've\": \"they_will_have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n",
    "    \"wasn't\": \"was_not\", \"we'd\": \"we_would\", \"we'd've\": \"we_would_have\", \"we'll\": \"we_will\", \n",
    "    \"we'll've\": \"we_will_have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were_not\", \"what'll\": \"what_will\", \n",
    "    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what_have\", \n",
    "    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "    \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will_not\", \n",
    "    \"won't've\": \"will_not_have\", \"would've\": \"would have\", \"wouldn't\": \"would_not\", \"wouldn't've\": \"would_not_have\", \n",
    "    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you_have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d64dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc568cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_words = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e263ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stop_words + punct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_easy_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85532020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(text):\n",
    "    text = str(text)\n",
    "    print('text :', text)\n",
    "    stripped = text.strip()\n",
    "    print('stripped :', stripped)\n",
    "    lowered = stripped.lower()\n",
    "    print('lowered :', lowered)\n",
    "    unurl = re.sub('http[s]?://\\S+', '', lowered)\n",
    "    print('unurl :', unurl)\n",
    "    for i in unurl.split():\n",
    "        if i[0]  in ['#','@']:\n",
    "            unurl = unurl.replace(i, ' ')\n",
    "    unhasg_unattherate = unurl\n",
    "    print('unhasg_unattherate :', unhasg_unattherate)\n",
    "    unspaced = ' '.join(unhasg_unattherate.split()).strip()\n",
    "    print('unspaced :', unspaced)\n",
    "    expanded = contractions.fix(unspaced)\n",
    "    print('expanded :', expanded)\n",
    "    digit_to_word = lambda x : re.sub('-|\\s+','',num2words(x.group()))\n",
    "    digit_to_word = re.sub(r'\\d+', digit_to_word, expanded)\n",
    "    print('digit_to_word : ', digit_to_word)\n",
    "    clean_words = []\n",
    "    for word in word_tokenize(digit_to_word):\n",
    "        if word not in stop:\n",
    "            pos = pos_tag([word])\n",
    "            clean_word = lemmatizer.lemmatize(word, get_easy_pos(pos[0][1]))\n",
    "            correct_word = spell.correction(clean_word)\n",
    "            clean_words.append(correct_word)\n",
    "    print('clean_word : ', clean_words)\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae435f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
